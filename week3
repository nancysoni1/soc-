#Neural network model for image classification
import pandas as pd
import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt

mnist = tf.keras.datasets.mnist

def ReLU(Z):
    return np.maximum(Z, 0)

def softmax(Z):
    expZ = np.exp(Z - np.max(Z))
    return expZ / expZ.sum(axis=0, keepdims=True)

class NN:
    def __init__(self, input_size, hidden_size, output_size, learning_rate=0.01):
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.output_size = output_size
        self.learning_rate = learning_rate

        self.W1 = np.random.randn(hidden_size, input_size) * 0.01
        self.B1 = np.zeros((hidden_size, 1))
        self.W2 = np.random.randn(output_size, hidden_size) * 0.01
        self.B2 = np.zeros((output_size, 1))

        self.AO = None
        self.Z1 = None
        self.A1 = None
        self.Z2 = None
        self.A2 = None
        self.dW2 = None
        self.dB2 = None
        self.dW1 = None
        self.dB1 = None
        self.dW3 = None

    def forward_propagation(self, X):
        self.Z1 = np.dot(self.W1, X) + self.B1  # Linear combination for hidden layer
        self.A1 = ReLU(self.Z1)  # Activation for hidden layer
        self.Z2 = np.dot(self.W2, self.A1) + self.B2  # Linear combination for output layer
        self.A2 = softmax(self.Z2)  # Activation for output layer

    def one_hot(self, y):
        one_hot_encoded_data = np.zeros((self.output_size, y.size))
        one_hot_encoded_data[y, np.arange(y.size)] = 1
        return one_hot_encoded_data

    def backward_propagation(self, X, y):
        m = X.shape[1]
        Y = self.one_hot(y)

        dZ2 = self.A2 - Y
        self.dW2 = (1 / m) * np.dot(dZ2, self.A1.T)
        self.dB2 = (1 / m) * np.sum(dZ2, axis=1, keepdims=True)

        dA1 = np.dot(self.W2.T, dZ2)
        dZ1 = dA1 * (self.Z1 > 0)
        self.dW1 = (1 / m) * np.dot(dZ1, X.T)
        self.dB1 = (1 / m) * np.sum(dZ1, axis=1, keepdims=True)

    def update_params(self):
        self.W1 -= self.learning_rate * self.dW1
        self.B1 -= self.learning_rate * self.dB1
        self.W2 -= self.learning_rate * self.dW2
        self.B2 -= self.learning_rate * self.dB2

    def get_predictions(self, X):
        self.forward_propagation(X)
        return np.argmax(self.A2, axis=0)#argmax return the index of maximum element

    def get_accuracy(self, predictions, labels):
        return np.mean(predictions == labels)

    def gradient_descent(self, X, y, iters=1000):
        for i in range(iters):
            self.forward_propagation(X)
            self.backward_propagation(X, y)
            self.update_params()
            if i % 100 == 0:
                predictions = self.get_predictions(X)
                accuracy = self.get_accuracy(predictions, y)
                print(f'Iteration {i}, Accuracy: {accuracy:.4f}')

    def cross_entropy_loss(self, X, y):
        m = X.shape[1]
        Y = self.one_hot(y)
        log_probs = -np.log(self.A2[Y, np.arange(m)])
        return np.sum(log_probs) / m

    def show_predictions(self, X, y, num_samples=10):
        random_indices = np.random.randint(0, X.shape[1], size=num_samples)
        for index in random_indices:
            sample_image = X[:, index].reshape((28, 28))
            plt.imshow(sample_image, cmap='gray')
            plt.title(f"Actual: {y[index]}, Predicted: {self.get_predictions(X[:, [index]])[0]}")
            plt.show()

(X_train, Y_train), (X_test, Y_test) = mnist.load_data()

miu = np.mean(X_train, axis=(0, 1), keepdims=True)
stds = np.std(X_train, axis=(0, 1), keepdims=True)

mius = np.mean(X_test, axis=(0, 1), keepdims=True)
stdse = np.std(X_test, axis=(0, 1), keepdims=True)

X_normal_train = (X_train - miu) / (stds + 1e-7)
X_normal_test = (X_test - mius) / (stdse + 1e-7)

X_normal_train = X_normal_train.reshape((60000, -1)).T
X_normal_test = X_normal_test.reshape((10000, -1)).T

nn = NN(input_size=784, hidden_size=128, output_size=10, learning_rate=0.01)
nn.gradient_descent(X_normal_train, Y_train, iters=1000)

predictions = nn.get_predictions(X_normal_test)
accuracy = nn.get_accuracy(predictions, Y_test)
print(f'Test Accuracy: {accuracy:.4f}')

nn.show_predictions(X_normal_test, Y_test, num_samples=10)





#RNN model
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from statsmodels.tsa.seasonal import seasonal_decompose
from sklearn.preprocessing import MinMaxScaler
from keras.preprocessing.sequence import TimeseriesGenerator
from keras.models import Sequential
from keras.layers import Dense, LSTM
from sklearn.metrics import mean_squared_error
from math import sqrt

# Load the dataset
#sunspots_url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/monthly-sunspots.csv'
df = pd.read_csv(sunspots_url)

# Correct the index
df['Month'] = pd.to_datetime(df['Month'])
df.set_index('Month', inplace=True)

# Decompose the time series
results = seasonal_decompose(df['Sunspots'], model='additive', period=12)
results.plot()
plt.show()

# Splitting the data into train and test sets
split_index = int(0.8 * len(df))
train = df.iloc[:split_index]
test = df.iloc[split_index:]

# Scaling the data
scaler = MinMaxScaler(feature_range=(0, 1))
scaler.fit(train)
scaled_train = scaler.transform(train)
scaled_test = scaler.transform(test)

# Time series generator
n_input = 12
n_features = 1
generator = TimeseriesGenerator(scaled_train, scaled_train, length=n_input, batch_size=1)
X, y = generator[0]
print(f'Given the Array: \n{X.flatten()}')
print(f'Predict this y: \n {y}')

# Building and training the LSTM model
model = Sequential()
model.add(LSTM(100, activation='relu', input_shape=(n_input, n_features)))
model.add(Dense(1))
model.compile(optimizer='adam', loss='mse')
model.fit(generator, epochs=13)

# Plot loss per epoch
loss_per_epoch = model.history.history['loss']
plt.plot(range(len(loss_per_epoch)), loss_per_epoch)
plt.show()

# Making predictions
test_predictions = []

first_eval_batch = scaled_train[-n_input:]
current_batch = first_eval_batch.reshape((1, n_input, n_features))

for i in range(len(test)):
    current_pred = model.predict(current_batch)[0]
    test_predictions.append(current_pred)
    current_batch = np.append(current_batch[:, 1:, :], [[current_pred]], axis=1)

true_predictions = scaler.inverse_transform(test_predictions)

# Inverse scaling for the predictions
test['Predictions'] = true_predictions

# Plotting the results
test.plot(figsize=(14, 5))
plt.show()

# Calculating RMSE
rmse = sqrt(mean_squared_error(test['Sunspots'], test['Predictions']))
print(f'RMSE: {rmse}')
